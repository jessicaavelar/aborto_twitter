{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd55c29",
   "metadata": {},
   "source": [
    "**Ver o que tem sobre o assunto:**\n",
    "\n",
    "`1 - Az Minas;`\n",
    "\n",
    "`2 - Gênero e Número;`\n",
    "\n",
    "`3 - Twitter;`\n",
    "\n",
    "`4 - ver como é feita as análises na Novelo.`\n",
    "\n",
    "\n",
    "**O passo a passo:**\n",
    "\n",
    "1º - Pegar os tweets;\n",
    "\n",
    "    https://colab.research.google.com/drive/1QR013vOUjA6Wf4MPYFxh01vsZ91ynUcB?authuser=3#scrollTo=fC2vg0EgTCh8\n",
    "    \n",
    "    https://drive.google.com/drive/u/3/my-drive/tweets_aborto.csv\n",
    "    \n",
    "    \n",
    "\n",
    "2º - Fazer a limpeza;\n",
    "\n",
    "* **Excluir outras linguas;**\n",
    "* **Excluir duplicados;**\n",
    "* **Excluir 2022;**\n",
    "* **Stopwords;**\n",
    "* ...\n",
    "\n",
    "**3º - Gerar nuvem de palavras;**\n",
    "\n",
    "**4º - Separar base de treino;**\n",
    "\n",
    "**5º - Fazer análise de sentimento;**\n",
    "\n",
    "**6º - Gráfico das análises;**\n",
    "\n",
    "7º - Fazer o texto;\n",
    "\n",
    "8º - Entrevistar alguém;\n",
    "\n",
    "9º - Pedir meninas para revisarem;\n",
    "\n",
    "10º - Publicar no site da Carina;\n",
    "\n",
    "11º - Subir no GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver quais bibliotecas são do Python e mudar a ordem delas\n",
    "\n",
    "import altair as alt\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk import stem\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier, SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e7f6b",
   "metadata": {},
   "source": [
    "### Limpeza dos dados\n",
    "\n",
    "Aborto é uma palavra comum a muitos idiomas, é necessário fazer a limpeza dos dados e selecionar apenas os tweets em que a língua é \"português. Alám disso, ao coletar os dados do Twitter, vieram alguns repetidos e precisamos ficar com dados únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets_aborto.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edffb3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c714fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excluir tweets que não são em português\n",
    "\n",
    "df_remove = df.loc[(df[\"language\"] != \"pt\")]\n",
    "\n",
    "dados = df.drop(df_remove.index)\n",
    "\n",
    "dados.reset_index(drop = True)\n",
    "\n",
    "# Excluir duplicados\n",
    "\n",
    "dados = dados.drop_duplicates(inplace = False).reset_index(drop = True)\n",
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa803ce5",
   "metadata": {},
   "source": [
    "### Pré-processamento e tratamento dos dados\n",
    "\n",
    "Remover as palavras que não possuem relevância e a pontuação, eles não são usados no processamento da linguagem.\n",
    "\n",
    "Exemplo de stop words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados[\"tweet\"]\n",
    "\n",
    "texto = \"\"\n",
    "for i, r in dados.iterrows():\n",
    "    texto = texto + r[\"tweet\"] + \".\"\n",
    "    texto = texto.lower()\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5b2da",
   "metadata": {},
   "source": [
    "#### Tokenização\n",
    "\n",
    "Transformar as palavras em tokens únicos com o objetivo de fazer a contagem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizar_p = word_tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75c1c5",
   "metadata": {},
   "source": [
    "#### Distribuição\n",
    "\n",
    "Entender como está a distribuição das palavras para limpar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(tokenizar_p)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(30, cumulative = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed9fb3",
   "metadata": {},
   "source": [
    "#### Stopword\n",
    "\n",
    "Remover as palavras que não possui significado para o processamento de linguagem natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53e3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtrado = []\n",
    "\n",
    "for x in [\"q\", \"pra\", \"t\", \"co\", \"https\", \"vc\"]:\n",
    "    stop_words.add(x)\n",
    "    \n",
    "for p in tokenizar_p:\n",
    "    if p not in stop_words:\n",
    "        filtrado.append(p)\n",
    "\n",
    "token = RegexpTokenizer(r\"\\w+\")\n",
    "palavras = token.tokenize(\" \".join(filtrado))\n",
    "print(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5159c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(palavras)\n",
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d9e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist.plot(30, cumulative = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar em dataframe\n",
    "\n",
    "df_palavras = fdist.most_common(40)\n",
    "df_palavras = pd.DataFrame(df_palavras, columns = [\"Palavras\", \"Quantidade\"]).sort_values(by=[\"Quantidade\"],\n",
    "                                                                                         ascending = False)\n",
    "# Excluir a palavra aborto\n",
    "df_palavras = df_palavras.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79c104",
   "metadata": {},
   "source": [
    "### Análise de sentimento\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a740e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para bag-of-words\n",
    "\n",
    "def divide(dados):\n",
    "    dados_new = []\n",
    "    for palavra in dados:\n",
    "        palavra_filter = [i.lower() for i in palavra.split()]\n",
    "        dados_new.append(palavra_filter)\n",
    "    return dados_new\n",
    "\n",
    "def bag_of_words(palavras):\n",
    "    return dict([(palavra, palavras.count(palavra)) for palavra in palavras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar os classificadores\n",
    "\n",
    "def treina_classificadores(positivo, negativo, neutro):\n",
    "    posdados = []\n",
    "    with open(positivo, \"r\", encoding = \"utf-8\") as myfile:\n",
    "        ler = csv.reader(myfile, delimiter = \",\")\n",
    "        for val in ler:\n",
    "            if len(val) > 0:\n",
    "                posdados.append(val[0])\n",
    "            \n",
    "    negdados = []\n",
    "    with open(negativo, \"r\", encoding = \"utf-8\") as myfile:\n",
    "        ler = csv.reader(myfile, delimiter = \",\")\n",
    "        for val in ler:\n",
    "            if len(val) > 0:\n",
    "                negdados.append(val[0])\n",
    "    \n",
    "    neudados = []\n",
    "    with open(neutro, \"r\", encoding = \"utf-8\") as myfile:\n",
    "        ler = csv.reader(myfile, delimiter = \",\")\n",
    "        for val in ler:\n",
    "            if len(val) > 0:\n",
    "                neudados.append(val[0])\n",
    "    \n",
    "    contra = [(bag_of_words(f), \"contra\") for f in divide(negdados)]\n",
    "    pro = [(bag_of_words(f), \"a_favor\") for f in divide(posdados)]\n",
    "    outros = [(bag_of_words(f), \"outros\") for f in divide(neudados)]\n",
    "    treino = contra + pro + outros\n",
    "    \n",
    "    classificadorME = MaxentClassifier.train(treino, 'GIS', trace=0, encoding=None, labels=None,\n",
    "                                             gaussian_prior_sigma=0, max_iter = 1)\n",
    "    \n",
    "    classificadorSVM = SklearnClassifier(LinearSVC(), sparse=False)\n",
    "    classificadorSVM.train(treino)\n",
    "    \n",
    "    classificadorNB = NaiveBayesClassifier.train(treino)\n",
    "    \n",
    "    return ([classificadorME, classificadorSVM, classificadorNB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificar\n",
    "\n",
    "def classifica(sentencas, classificadores):\n",
    "    ret = []\n",
    "    for s in sentencas:\n",
    "        c = divide([s])\n",
    "        feats = bag_of_words(c[0])\n",
    "        classificacao = []\n",
    "        classificacao.append(classificadores[1].classify(feats))\n",
    "        classificacao.append(classificadores[2].classify(feats))\n",
    "        classificacao.append(classificadores[0].classify(feats))\n",
    "        ret.append(classificacao)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de treino com amostras dos tweets nas polaridades\n",
    "\n",
    "contra = \"tweets_contra.csv\"\n",
    "a_favor = \"tweets_a_favor.csv\"\n",
    "outros = \"tweets_outros.csv\"\n",
    "\n",
    "classificadores = treina_classificadores(a_favor, contra, outros)\n",
    "\n",
    "\n",
    "# CLassificação\n",
    "\n",
    "tweets = dados[\"tweet\"]\n",
    "\n",
    "classificacao = classifica(tweets, classificadores)\n",
    "\n",
    "\n",
    "# DataFrame com as classificações\n",
    "\n",
    "ME = []\n",
    "SVM = []\n",
    "NB = []\n",
    "\n",
    "for cla in classificacao:\n",
    "    ME.append(cla[0])\n",
    "    SVM.append(cla[1])\n",
    "    NB.append(cla[2])\n",
    "    \n",
    "df = pd.DataFrame(list(zip(tweets, ME, SVM, NB)), columns = [\"tweet\", \"ME\", \"SVM\", \"NB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4700495",
   "metadata": {},
   "source": [
    "#### Teste para ver qual dos classificadores funcionou melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09853e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4230f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"tweet\"].loc[7879])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752929a",
   "metadata": {},
   "source": [
    "A partir da análise, foi possível perceber que o Maxima Entropia (ME) é o que melhor funciona para a nossa amostra, isso porque, de acordo com os testes feitos, ele conseguiu captar melhor que os outros classificadores quando era ironia ou xingamento.\n",
    "\n",
    "Das 50 amostras analisadas:\n",
    "\n",
    "* Os três classificadores erraram em 4 afirmações.\n",
    "* Máxima Entropia (ME) - acertou 39;\n",
    "* Support Vector Machine (SVM) - acertou 25;\n",
    "* Naive Bayes (NB) - acertou 36.\n",
    "\n",
    "\n",
    "**Exemplos**\n",
    "\n",
    "*Realidade dura: isso não é aborto, é outra coisa lamentável mas não é o absurdo do aborto*\n",
    "\n",
    "* ME classificou como \"contra\", SVM e NB classificaram como \"a favor\".\n",
    "\n",
    "*Vocês apoiam o aborto?*\n",
    "\n",
    "* ME classificou como \"outros\", SVM classificou \"contra\" e NB classificaram como \"a favor\".\n",
    "\n",
    "*Falling é uma oneshot inspirada na música do Harry Styles que conta a história de um casal que acredita ser pra sempre um na vida do outro. No entanto a vida parece não colaborar com o desejo dos dois e tudo parece desmoronar  (Contém menção de aborto)*\n",
    "\n",
    "* ME classificou como \"outros\", SVM e NB classificaram como \"a favor\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem:\n",
    "\n",
    "df[\"ME\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e991048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SVM\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ec41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NB\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes com tweets a favor e outro com tweets contra\n",
    "\n",
    "pro = df.query(\"ME == 'a_favor'\")\n",
    "pro = pro.drop(columns = [\"SVM\", \"NB\"])\n",
    "\n",
    "contra = df.query(\"ME == 'contra'\")\n",
    "contra = contra.drop(columns = [\"SVM\", \"NB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c107631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras mais comuns a favor do aborto:\n",
    "\n",
    "pro[\"tweet\"]\n",
    "\n",
    "texto_pro = \"\"\n",
    "for i, r in pro.iterrows():\n",
    "    texto_pro = texto_pro + r[\"tweet\"] + \".\"\n",
    "    texto_pro = texto_pro.lower()\n",
    "\n",
    "token_pro = word_tokenize(texto_pro)\n",
    "\n",
    "filtrado_pro = []\n",
    "    \n",
    "for p in token_pro:\n",
    "    if p not in stop_words:\n",
    "        filtrado_pro.append(p)\n",
    "\n",
    "token_pro = RegexpTokenizer(r\"\\w+\")\n",
    "palavras_pro = token_pro.tokenize(\" \".join(filtrado_pro))\n",
    "\n",
    "fdist_pro = FreqDist(palavras_pro)\n",
    "df_pro = fdist_pro.most_common(10)\n",
    "df_pro = pd.DataFrame(df_pro, columns = [\"Palavras\", \"Quantidade\"]).sort_values(by=[\"Quantidade\"],\n",
    "                                                                                         ascending = False)\n",
    "# Excluir a palavra aborto\n",
    "df_pro = df_pro.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07669cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_pro = FreqDist(palavras_pro)\n",
    "df_pro = fdist_pro.most_common(20)\n",
    "df_pro = pd.DataFrame(df_pro, columns = [\"Palavras\", \"Quantidade\"]).sort_values(by=[\"Quantidade\"],\n",
    "                                                                                         ascending = False)\n",
    "# Excluir a palavra aborto\n",
    "df_pro = df_pro.drop(0)\n",
    "df_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras mais comuns contra do aboro:\n",
    "\n",
    "contra[\"tweet\"]\n",
    "\n",
    "texto_contra = \"\"\n",
    "for i, r in contra.iterrows():\n",
    "    texto_contra = texto_contra + r[\"tweet\"] + \".\"\n",
    "    texto_contra = texto_contra.lower()\n",
    "\n",
    "token_contra = word_tokenize(texto_contra)\n",
    "\n",
    "filtrado_contra = []\n",
    "    \n",
    "for p in token_contra:\n",
    "    if p not in stop_words:\n",
    "        filtrado_contra.append(p)\n",
    "\n",
    "token_contra = RegexpTokenizer(r\"\\w+\")\n",
    "palavras_contra = token_contra.tokenize(\" \".join(filtrado_contra))\n",
    "\n",
    "fdist_contra = FreqDist(palavras_contra)\n",
    "df_contra = fdist_contra.most_common(10)\n",
    "df_contra = pd.DataFrame(df_contra, columns = [\"Palavras\", \"Quantidade\"]).sort_values(by=[\"Quantidade\"],\n",
    "                                                                                         ascending = False)\n",
    "# Excluir a palavra aborto\n",
    "df_contra = df_contra.drop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_contra = FreqDist(palavras_contra)\n",
    "df_contra = fdist_contra.most_common(20)\n",
    "df_contra = pd.DataFrame(df_contra, columns = [\"Palavras\", \"Quantidade\"]).sort_values(by=[\"Quantidade\"],\n",
    "                                                                                         ascending = False)\n",
    "# Excluir a palavra aborto\n",
    "df_contra = df_contra.drop(0)\n",
    "df_contra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e6bca",
   "metadata": {},
   "source": [
    "### Visualizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4049af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuvem de palavras\n",
    "\n",
    "wordclound = WordCloud(stopwords = stop_words,\n",
    "                      # background_color = \"white\",\n",
    "                    #   colormap=\"Purples\",\n",
    "                       width = 3000,\n",
    "                       height = 2000,\n",
    "                       max_words = 300,\n",
    "                      ).generate(texto)\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(wordclound, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de palavras - geral\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "alt.Chart(df_palavras).mark_bar(size = 15, opacity=0.8).encode(\n",
    "    x = alt.X(\"Palavras\", title = \"Palavras\"),\n",
    "    y = alt.Y(\"Quantidade\", title = \"Frequência\"),\n",
    "    tooltip = [\"Palavras\", \"Quantidade\"],\n",
    "    color = alt.Color(\"Palavras\", legend=None) \n",
    ").properties(width = 800, title = 'Principais palavras dos tweets e frequência que aparecem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de palavras - a favor\n",
    "\n",
    "alt.Chart(df_pro).mark_bar(size = 20, opacity=0.6).encode(\n",
    "    x = alt.X(\"Quantidade\", title = \"Frequência\"),\n",
    "    y = alt.Y(\"Palavras\", title = \"Palavras\"),\n",
    "    tooltip = [\"Palavras\", \"Quantidade\"],\n",
    "    color = alt.value(\"purple\") \n",
    ").properties(height = 300, title = 'Principais palavras nos tweets classificados como favoráveis à legalização do aborto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77821bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de palavras - contra\n",
    "\n",
    "alt.Chart(df_contra).mark_bar(size = 20, opacity=0.7).encode(\n",
    "    x = alt.X(\"Quantidade\", title = \"Frequência\"),\n",
    "    y = alt.Y(\"Palavras\", title = \"Palavras\"),\n",
    "    tooltip = [\"Palavras\", \"Quantidade\"],\n",
    "    color = alt.value(\"red\"), #legend=None) \n",
    ").properties(height = 300, title = 'Principais palavras nos tweets classificados como contrários à legalização do aborto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico análise de sentimento\n",
    "\n",
    "alt.Chart(df).mark_bar(size = 50, opacity=0.8).encode(\n",
    "    x = alt.X(\"ME\", title = \"\"),\n",
    "    y = alt.Y(\"count()\", title = \"Quantidade de tweets\"),\n",
    "    tooltip = [\"count()\"],\n",
    "    color = alt.Color(\"ME\", legend = alt.Legend(title = \"Posicionamento\"))\n",
    ").properties(width = 350, title = 'Análise de sentimento de tweets sobre \"aborto\"')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
